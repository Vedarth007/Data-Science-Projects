# -*- coding: utf-8 -*-
"""NLP-1/NLP_Final_using_model_sentence_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GxoKF6Q7YA977qLeG6A5yhDruAql3ut9
"""

!pip install sentence-transformers transformers faiss-cpu

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline

# Initialize the models
embedding_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')
qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")

# Sample lecture notes
lecture_notes = [
    "Lecture 1: Introduction to Machine Learning. Machine learning is a method of data analysis that automates analytical model building.",
    "Lecture 2: Deep Learning. Deep learning is a subset of machine learning where artificial neural networks learn from large amounts of data."
]

# Sample LLM architectures table
llm_architectures = [
    {"name": "BERT", "description": "BERT is a transformer-based model for NLP tasks."},
    {"name": "GPT-3", "description": "GPT-3 is an autoregressive language model that uses deep learning to produce human-like text."}
]

# Combine all documents for embedding
documents = lecture_notes + [arch["description"] for arch in llm_architectures]

# Compute document embeddings
document_embeddings = embedding_model.encode(documents, convert_to_tensor=True)

# Initialize FAISS index
dimension = document_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)

# Convert to numpy array and add to index
faiss_index = faiss.IndexIDMap(index)
faiss_index.add_with_ids(document_embeddings.cpu().numpy(), np.array(range(len(documents))))

def answer_query(query, k=3):
    # Generate query embedding
    query_embedding = embedding_model.encode(query, convert_to_numpy=True)

    # Ensure the embedding is in the correct shape
    query_embedding = query_embedding.reshape(1, -1)

    # Search FAISS index
    D, I = faiss_index.search(query_embedding, k)

    # Retrieve top documents
    top_docs = [documents[idx] for idx in I[0]]

    # Use QA model if detailed answer is needed from context
    context = " ".join(top_docs)
    result = qa_pipeline(question=query, context=context)

    return result

# Example query
query = "What is machine learning?"
result = answer_query(query)

print("Answer:")
print(result)





# Example query
query = "What is deep learning?"
result = answer_query(query)

print("Answer:")
print(result)

# Example query
query = "What is BERT?"
result = answer_query(query)

print("Answer:")
print(result)

# Example query
query = "What is machine learning and deep learning?"
result = answer_query(query)

print("Answer:")
print(result)

